{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19a902a5-c3c0-41e4-97bd-92a7e105b4c6",
   "metadata": {},
   "source": [
    "# ü¶ô1Ô∏è‚É£: Convert PDFs into Images\n",
    "pdf2image: https://pypi.org/project/pdf2image/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7240d2-e2fc-4a85-8399-591fc5be25f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pdf2image import convert_from_path\n",
    "# import fitz\n",
    "\n",
    "first_page_only = True\n",
    "\n",
    "pdf_folder = '/home/exouser/Chat-with-your-Research-Articles-LLM-Retrieval-Augmented-Generation/pdfs'\n",
    "image_folder = '/home/exouser/Chat-with-your-Research-Articles-LLM-Retrieval-Augmented-Generation/images'\n",
    "\n",
    "if not os.path.exists(image_folder):\n",
    "    os.makedirs(image_folder)\n",
    "\n",
    "for filename in os.listdir(pdf_folder):\n",
    "    print(f'filename: {filename}\\n')\n",
    "    if filename.endswith('.pdf'):\n",
    "        file_path = os.path.join(pdf_folder, filename)\n",
    "        print(f'file_path: {file_path}\\n')\n",
    "        \n",
    "        output_folder = os.path.join(image_folder, os.path.splitext(filename)[0])\n",
    "        print(f'output_folder: {output_folder}\\n')\n",
    "        \n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "        images = convert_from_path(file_path)\n",
    "        \n",
    "        for i, image in enumerate(images):\n",
    "            image_filename = f\"page_{i+1:03d}.jpeg\"\n",
    "            image_path = os.path.join(output_folder, image_filename)\n",
    "            image.save(image_path, 'JPEG')\n",
    "            print(f'image_path: {image_path}\\n')\n",
    "            \n",
    "        if first_page_only: \n",
    "            break\n",
    "    if first_page_only: \n",
    "        break\n",
    "\n",
    "print('PDFs converted to images!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afff536-9e28-465d-8745-2fe96e8c3c08",
   "metadata": {},
   "source": [
    "# ü¶ô2Ô∏è‚É£: Extract Text from Images, Join on Newlines, and Sentence Tokenize\n",
    "pytesseract: https://pypi.org/project/pytesseract/\n",
    "\n",
    "PIL: https://pypi.org/project/pillow/\n",
    "\n",
    "nltk: https://www.nltk.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baea0cb-f028-459c-bffe-982fc5ac420c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import pandas as pd\n",
    "\n",
    "first_page_only = False\n",
    "sentence_data = []\n",
    "\n",
    "first_loop = True\n",
    "for dir, subdirs, files in os.walk(image_folder):\n",
    "    if first_loop:\n",
    "        # walk first loop is current directory, nothing in it\n",
    "        first_loop = False\n",
    "        continue\n",
    "        \n",
    "    #print(f'dir: {dir}, subdirs: {subdirs}, files: {files}\\n')\n",
    "    for page_num, file in enumerate(sorted(files)):\n",
    "        #print(f'file: {file}\\n')\n",
    "        \n",
    "        if file.endswith('.jpeg'):\n",
    "            doc_name = dir.split('/')[-1]\n",
    "            #processed_text_doc['doc_name'] = doc_name\n",
    "            page = file.split('.')[0].split('_')[-1]\n",
    "            print(f'doc_name: {doc_name}, page: {page}\\n')\n",
    "            \n",
    "            file_path = os.path.join(dir, file)\n",
    "            image = Image.open(file_path)\n",
    "            extracted_text = pytesseract.image_to_string(image)\n",
    "            print('\\nüü† EXTRACTED:')\n",
    "            print(f'{extracted_text.replace(\"\\n\",\"‚û°Ô∏è\\n\")}\\n')\n",
    "\n",
    "            joined_extracted_text = extracted_text.replace(\"\\n\",\" \")\n",
    "            print('\\nüü° JOINED:')\n",
    "            print(f'{joined_extracted_text.replace(\"\\n\",\"‚û°Ô∏è\\n\")}\\n')\n",
    "            \n",
    "            tokenized_sentences = nltk.sent_tokenize(joined_extracted_text)\n",
    "            print('\\nüü¢ TOKENIZED:')\n",
    "            print(*[sentence for sentence in tokenized_sentences], sep='\\n\\n')\n",
    "\n",
    "            for sentence_num, sentence in enumerate(tokenized_sentences):\n",
    "                sentence_data.append({\"doc_name\": doc_name, \"page_num\": page_num + 1, \"sentence_num\": sentence_num + 1, \"sentence\": sentence})\n",
    "            #processed_text_doc['page_tokenized_sentences'] = tokenized_sentences\n",
    "                \n",
    "            if first_page_only: \n",
    "                break\n",
    "                \n",
    "    #processed_text_doc_list.append(processed_text_doc)\n",
    "                \n",
    "    if first_page_only and not first_loop: \n",
    "        break\n",
    "        \n",
    "sentence_df = pd.DataFrame(sentence_data)                                  \n",
    "print('Conversion complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c2a463-94d9-4415-bfb1-ea355bdcd770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "if not first_page_only:\n",
    "    sentence_df.to_csv(\"processed_text_doc_list.csv\", index=False)\n",
    "display(sentence_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dcbeae-cce9-431d-b904-815cc0b42496",
   "metadata": {},
   "source": [
    "# ü¶ô3Ô∏è‚É£: Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53740e64-7a52-4827-8142-cc98ca0ddb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "sentence_df = pd.read_csv(\"processed_text_doc_list.csv\")\n",
    "documents = sentence_df['sentence'].tolist()\n",
    "\n",
    "ids = []\n",
    "metadatas = []\n",
    "for index, row in sentence_df.iterrows():\n",
    "    ids.append(str(index))\n",
    "    metadatas.append({\n",
    "        \"doc_name\": row['doc_name'],\n",
    "        \"page_num\": row['page_num'],\n",
    "        \"sentence_num\": row['sentence_num']\n",
    "    })\n",
    "\n",
    "client = chromadb.Client()\n",
    "#client.delete_collection(name=\"docs\")\n",
    "collection = client.create_collection(name=\"docs\", get_or_create=True)\n",
    "\n",
    "\n",
    "collection.add(\n",
    "    ids = ids,\n",
    "    documents = documents,\n",
    "    metadatas = metadatas,\n",
    ")\n",
    "\n",
    "print(\"embeddings created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90686f4-7bc8-4daa-a34b-9d3d196878de",
   "metadata": {},
   "source": [
    "# ü¶ô4Ô∏è‚É£: Prompt Ollama Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a04b3f-0362-42a3-8c2e-c116620dfe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_ollama(llm_model, system_prompt = '', user_prompt = '', temperature = 0.8, stream = True):\n",
    "    response  = ollama.chat(\n",
    "        model = llm_model,\n",
    "        messages = [{\n",
    "            'role': 'system', \n",
    "            'content': system_prompt,\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': user_prompt,\n",
    "        }],\n",
    "        options = {'temperature': temperature},\n",
    "        stream = stream,\n",
    "    )\n",
    "\n",
    "    if stream:\n",
    "        for chunk in response :\n",
    "            print(chunk['message']['content'], end = '', flush=True)\n",
    "            return chunk['message']['content']\n",
    "    else:\n",
    "        print(response ['message']['content'])\n",
    "        return response ['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936ff8d0-935d-44cb-8619-19914860d5cb",
   "metadata": {},
   "source": [
    "# ü¶ô5Ô∏è‚É£: Query Embeddings, get Alternative Queries from LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2878d320-ef88-4b9c-821d-00977648358d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import json\n",
    "\n",
    "llm_model = 'llama3:8b'\n",
    "\n",
    "system_prompt = \"Provide three alternative ways to ask the given query to maximize relevant semantic search results. Write only the 3 sentences separated with one new line and no additional text or numbering.\"\n",
    "\n",
    "query = \"What is the typical length of a dream?\"\n",
    "\n",
    "temperature = 0.5\n",
    "\n",
    "stream = False\n",
    "\n",
    "response = prompt_ollama(llm_model, system_prompt, query, temperature, stream)\n",
    "\n",
    "filtered_response = [item for item in response.split('\\n') if item != '']\n",
    "\n",
    "results_per_prompt = 15\n",
    "\n",
    "results_list = []\n",
    "for item in filtered_response:\n",
    "    print(item)\n",
    "    results_list.append(collection.query(\n",
    "        query_texts=item,\n",
    "        n_results=results_per_prompt\n",
    "    ))\n",
    "    \n",
    "\n",
    "combined_documents_lists = []\n",
    "combined_doc_name_lists = []\n",
    "combined_page_num_lists = []\n",
    "combined_sentence_num_lists = []\n",
    "combined_distance_lists = []\n",
    "for result in results_list:\n",
    "    #print(json.dumps(result, indent=1))\n",
    "    combined_documents_lists += result['documents'][0]\n",
    "    combined_distance_lists += result['distances'][0]\n",
    "    for metadata in result['metadatas'][0]:\n",
    "        combined_doc_name_lists += [metadata['doc_name']]\n",
    "        combined_page_num_lists += [metadata['page_num']]\n",
    "        combined_sentence_num_lists += [metadata['sentence_num']]\n",
    "\n",
    "response_df = pd.DataFrame({\n",
    "    'documents': combined_documents_lists,\n",
    "    'doc_name': combined_doc_name_lists,\n",
    "    'page_num': combined_page_num_lists,\n",
    "    'sentence_num' : combined_sentence_num_lists,\n",
    "    'distances': combined_distance_lists\n",
    "})\n",
    "\n",
    "display(response_df)\n",
    "df_unique_responses = response_df.drop_duplicates(subset='documents').reset_index(drop=True)\n",
    "display(df_unique_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0ab862-baeb-4080-ae1d-a7aad4ee78b2",
   "metadata": {},
   "source": [
    "# ü¶ô6Ô∏è‚É£: Get Surrounding Sentences for each Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502423b5-ed92-487c-b565-e740256f41c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get concatenated sentences 3 above and 3 below\n",
    "def get_surrounding_sentences_concatenated(row, all_sentences_df, window=3):\n",
    "    doc_name = row['doc_name']\n",
    "    page_num = row['page_num']\n",
    "    sentence_num = row['sentence_num']\n",
    "    \n",
    "    # Filter the full dataframe for the same document and page\n",
    "    filtered_df = all_sentences_df[(all_sentences_df['doc_name'] == doc_name) & \n",
    "                                   (all_sentences_df['page_num'] == page_num)]\n",
    "    \n",
    "    # Get the indices of sentences within the window\n",
    "    surrounding_indices = range(sentence_num - window, sentence_num + window + 1)\n",
    "    \n",
    "    # Get the surrounding sentences\n",
    "    surrounding_sentences = filtered_df[filtered_df['sentence_num'].isin(surrounding_indices)]\n",
    "    \n",
    "    # Concatenate the sentences into a single string\n",
    "    concatenated_sentences = ' '.join(surrounding_sentences['sentence'].tolist())\n",
    "    \n",
    "    # Return the concatenated sentences and the first sentence number in the group\n",
    "    return pd.Series([concatenated_sentences, surrounding_sentences['sentence_num'].iloc[0]])\n",
    "\n",
    "# Apply the function to each row in df_unique_responses and create new columns\n",
    "df_unique_responses[['surrounding_sentences', 'first_sentence_num']] = df_unique_responses.apply(\n",
    "    lambda row: get_surrounding_sentences_concatenated(row, sentence_df), axis=1\n",
    ")\n",
    "\n",
    "# Adjust the df_unique_responses to have the correct values\n",
    "df_unique_responses['sentence_num'] = df_unique_responses['first_sentence_num']\n",
    "df_unique_responses.drop(columns=['first_sentence_num'], inplace=True)\n",
    "\n",
    "# Display the df_unique_responses with the new columns\n",
    "#print(df_unique_responses)\n",
    "\n",
    "# Ensure expanded_list is a list of dictionaries\n",
    "expanded_list = [entry for entry in df_unique_responses.to_dict(orient='records')]\n",
    "#print(expanded_list)\n",
    "\n",
    "# Display results\n",
    "combined_sentences = [entry['surrounding_sentences'] for entry in expanded_list]\n",
    "\n",
    "prompt_data = ''\n",
    "for i, text in enumerate(combined_sentences):\n",
    "    prompt_data += f\"[{i+1}] {text}\\n\"\n",
    "\n",
    "# Display prompt data\n",
    "print(prompt_data)\n",
    "\n",
    "# Display results with metadata\n",
    "for i, entry in enumerate(expanded_list):\n",
    "    distance = entry['distances']\n",
    "    #print(f'\\n[{i+1}][Distance:{distance:.3f}, Document: {entry[\"doc_name\"]}, Page: {entry[\"page_num\"]}, Sentence: {entry[\"sentence_num\"]}]\\n{entry[\"surrounding_sentences\"]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4955b77f-e001-44cc-8532-218f2d99e0aa",
   "metadata": {},
   "source": [
    "# ü¶ô7Ô∏è‚É£: Prompt LLM with Query and Provided Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e0d3ff-fe1e-4a8e-a56a-365daeabc7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"Answer the query using only the provided information\\n\\nDo your best to completely and succinctly answer the query, some provided data may not be relevant to the query and does not need to be included. Don't provide information not relvant to the query. Include the number of the reference data e.g. [1], [2], etc. Also keep in mind that this is extracted from a PDF and may have lines inserted that aren't part of the narrative.\" \n",
    "user_prompt = f\"Query: {query}\\n\\nProvided information:\\n\\n{prompt_data}\"\n",
    "\n",
    "print(f'query: {query}\\n')\n",
    "stream = ollama.chat(\n",
    "    model = llm_model,\n",
    "    messages = [{\n",
    "        'role': 'system', \n",
    "        'content': system_prompt,\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': user_prompt,\n",
    "    }],\n",
    "    options = {'temperature': 0.8},\n",
    "    stream = True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)\n",
    "\n",
    "print(\"\\n\\n[[Provided Data]]:\")\n",
    "\n",
    "for i, entry in enumerate(expanded_list):\n",
    "    print(entry)\n",
    "    distance = entry['distances']\n",
    "    print(f'\\n[{i+1}][Distance:{distance:.3f}, Document: {entry['doc_name']}, Page: {entry['page_num']}, Sentence: {entry['sentence_num']}]\\n{entry['surrounding_sentences']}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
